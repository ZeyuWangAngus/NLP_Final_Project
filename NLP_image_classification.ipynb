{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2b9fc56-a60a-42cb-9058-4e10b3b77b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5f6dac8d-0ce5-41f9-9bb4-d39f9419ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image,ImageOps\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "27dd863a-7ced-487d-80a9-745f0ad37960",
   "metadata": {},
   "outputs": [],
   "source": [
    "imag1 = Image.open('hate/bringPain.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4376fc67-ce43-4b87-bd7d-de0dcc44369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imag1 = imag1.resize((1000,1000))\n",
    "imag1.save('1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3e928e32-d840-488d-b280-d70532f269f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(img, expected_size):\n",
    "    desired_size = expected_size\n",
    "    delta_width = desired_size - img.size[0]\n",
    "    delta_height = desired_size - img.size[1]\n",
    "    pad_width = delta_width // 2\n",
    "    pad_height = delta_height // 2\n",
    "    padding = (pad_width, pad_height, delta_width - pad_width, delta_height - pad_height)\n",
    "    return ImageOps.expand(img, padding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0089df6d-f066-48c9-9f35-11830d3dc090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_with_padding(img, expected_size):\n",
    "    img.thumbnail((expected_size[0], expected_size[1]))\n",
    "    # print(img.size)\n",
    "    delta_width = expected_size[0] - img.size[0]\n",
    "    delta_height = expected_size[1] - img.size[1]\n",
    "    pad_width = delta_width // 2\n",
    "    pad_height = delta_height // 2\n",
    "    padding = (pad_width, pad_height, delta_width - pad_width, delta_height - pad_height)\n",
    "    return ImageOps.expand(img, padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4f4f900-3e66-4ae0-84d0-0933f8984545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "hate_data = [f for f in listdir('hate') if isfile(join('hate', f))]\n",
    "nothate_data = [f for f in listdir('NotHate') if isfile(join('NotHate', f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9473b1eb-9c85-4826-adc8-c0bd91eb5f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add whole path\n",
    "hate_data_w = np.array(['hate/'+i for i in hate_data])\n",
    "nothate_data_w = np.array(['NotHate/'+i for i in nothate_data])\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(hate_data_w)\n",
    "np.random.shuffle(nothate_data_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3a242693-469b-46e6-8961-f37dda3597ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_number_hate = int(len(hate_data_w)*0.8)\n",
    "train_number_nothate = int(len(nothate_data_w)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fbb0b857-4f04-4168-93e4-ea5af666165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hate = hate_data_w[:train_number_hate]\n",
    "train_nothate = nothate_data_w[:train_number_nothate]\n",
    "test_hate = hate_data_w[train_number_hate:]\n",
    "test_nothate = nothate_data_w[train_number_nothate:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451de8de-49b7-4034-aff0-0e34be4ef4ac",
   "metadata": {},
   "source": [
    "## Resize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e80764e5-86fa-458a-a4ff-65fae690ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_hate:\n",
    "    imag1 = Image.open(i)\n",
    "    imag1 = resize_with_padding(imag1, (1000, 1000))\n",
    "    imag1.save(\"train/\"+i, 'PNG')\n",
    "for i in train_nothate:\n",
    "    imag1 = Image.open(i)\n",
    "    imag1 = resize_with_padding(imag1, (1000, 1000))\n",
    "    imag1.save(\"train/\"+i, 'PNG')\n",
    "for i in test_hate:\n",
    "    imag1 = Image.open(i)\n",
    "    imag1 = resize_with_padding(imag1, (1000, 1000))\n",
    "    imag1.save(\"test/\"+i, 'PNG')\n",
    "for i in test_nothate:\n",
    "    imag1 = Image.open(i)\n",
    "    imag1 = resize_with_padding(imag1, (1000, 1000))\n",
    "    imag1.save(\"test/\"+i, 'PNG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce923d2-d271-46c0-abff-33af2bee7d8a",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d43b9ac4-015e-4774-a457-8170067b9255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
    "from tensorflow import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "326bd94a-f376-452e-b4c0-d3746dafe79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "random.set_seed(seed)\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9413eff1-3413-47a9-b32b-462475897ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(64, 3, 3, input_shape = (1000, 1000, 3), activation = 'relu'))\n",
    "classifier.add(Conv2D(32, 3, 3, activation = 'relu'))\n",
    "classifier.add(Conv2D(16, 3, 3, activation = 'relu'))\n",
    "classifier.add(MaxPool2D(2,2))\n",
    "classifier.add(Conv2D(16, 3, 3, activation = 'relu'))\n",
    "classifier.add(MaxPool2D(pool_size = (2,2)))\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(256, activation='relu'))\n",
    "classifier.add(Dense(128, activation='tanh'))\n",
    "classifier.add(Dense(256, activation='relu'))\n",
    "classifier.add(Dense(128, activation='relu'))\n",
    "classifier.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cea613f3-a986-4848-9cd7-132e2880b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "645903e4-550b-4231-9081-5f5aa529c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e13e9b69-d136-4bcc-9efb-7d19b7f9f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c2090b1f-0226-4e5c-a226-f13213813a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 608 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('train', target_size=(1000, 1000), batch_size=32, class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3c6eb2d-e63a-419b-a197-11745e9dd97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 160 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = train_datagen.flow_from_directory('test', target_size=(1000, 1000), batch_size=32, class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74856486-d7cf-4918-8a5a-c0ea6895870c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0f/brs6kj9d6bbftjh0sk1763yh0000gn/T/ipykernel_76362/1417619662.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  classifier.fit_generator(training_set, steps_per_epoch=19, epochs=20, validation_data=test_set, validation_steps=2500, shuffle=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 02:46:01.099985: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - ETA: 0s - loss: -16.4842 - accuracy: 0.0839"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 02:47:16.006984: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2500 batches). You may need to use the repeat() function when building your dataset.\n",
      "19/19 [==============================] - 94s 5s/step - loss: -16.4842 - accuracy: 0.0839 - val_loss: -53.9523 - val_accuracy: 0.0875\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 72s 4s/step - loss: -142.0953 - accuracy: 0.0888\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 71s 4s/step - loss: -544.7047 - accuracy: 0.0888\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 71s 4s/step - loss: -1454.6536 - accuracy: 0.0888\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 71s 4s/step - loss: -3180.1077 - accuracy: 0.0888\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 72s 4s/step - loss: -6052.3735 - accuracy: 0.0888\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 72s 4s/step - loss: -10455.5732 - accuracy: 0.0888\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 72s 4s/step - loss: -16760.7129 - accuracy: 0.0888\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 72s 4s/step - loss: -25293.9648 - accuracy: 0.0888\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 71s 4s/step - loss: -36568.5742 - accuracy: 0.0888\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 71s 4s/step - loss: -50790.0742 - accuracy: 0.0888\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 71s 4s/step - loss: -68451.1953 - accuracy: 0.0888\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 71s 4s/step - loss: -89861.5234 - accuracy: 0.0888\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 71s 4s/step - loss: -115417.5703 - accuracy: 0.0888\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 71s 4s/step - loss: -145403.5312 - accuracy: 0.0888\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 71s 4s/step - loss: -180183.6406 - accuracy: 0.0888\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 71s 4s/step - loss: -220260.1094 - accuracy: 0.0888\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 71s 4s/step - loss: -265729.1562 - accuracy: 0.0888\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 71s 4s/step - loss: -316883.9062 - accuracy: 0.0888\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 71s 4s/step - loss: -374557.9375 - accuracy: 0.0888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15d59d700>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(training_set, steps_per_epoch=19, epochs=20, validation_data=test_set, validation_steps=2500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f669f086-4bb0-435b-916c-45ac599371a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fc10b1-cff2-45fc-bbc9-039a5d7a5e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a70d90-a726-453c-b8eb-03d5ef240d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
